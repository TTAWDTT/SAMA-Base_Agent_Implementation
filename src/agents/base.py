# ==============================================================================
# AI Agent åŸºç¡€å®ç° / AI Agent Base Implementation
# ==============================================================================
# å®ç°åŸºäºLLMçš„Agentå¾ªç¯ï¼Œé€šè¿‡å·¥å…·å®Œæˆä»»åŠ¡
# Implements LLM-based Agent loop, completing tasks through tools
#
# è®¾è®¡ç†å¿µ / Design Philosophy:
# - Agentæ˜¯ä¸€ä¸ªåœ¨å¾ªç¯ä¸­ä½¿ç”¨å·¥å…·æ¥å®ç°ç›®æ ‡çš„LLM
# - Agent can handle complex tasks, but the implementation is usually simple
# - It's typically just an LLM using tools in a loop based on environment feedback
#
# å‚è€ƒ / Reference:
# - https://www.anthropic.com/engineering/building-effective-agents
# ==============================================================================

import json
import time
import os
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Type, Union

from openai import OpenAI

from src.core.config import get_config, Config
from src.core.logger import get_logger, init_logging
from src.core.memory import ConversationMemory, get_memory
from src.core.prompts import get_system_prompt
from src.core.schema import (
    AgentState,
    AgentStep,
    AgentResponse,
    ToolCall,
    ToolResult,
    ToolResultStatus,
)
from src.tools import DEFAULT_TOOLS, BaseTool
from src.utils.helpers import (
    format_tool_result,
    generate_request_id,
    estimate_tokens,
)

logger = get_logger("agents.base")


class BaseAgent:
    """
    åŸºç¡€Agentç±» / Base Agent Class
    
    å®ç°æ ¸å¿ƒçš„Agentå¾ªç¯é€»è¾‘ï¼š
    1. æ¥æ”¶ç”¨æˆ·è¾“å…¥
    2. è°ƒç”¨LLMè¿›è¡Œæ€è€ƒ
    3. æ ¹æ®LLMçš„å†³ç­–æ‰§è¡Œå·¥å…·
    4. å°†å·¥å…·ç»“æœåé¦ˆç»™LLM
    5. é‡å¤ç›´åˆ°ä»»åŠ¡å®Œæˆæˆ–è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
    
    Implements core Agent loop logic:
    1. Receive user input
    2. Call LLM for thinking
    3. Execute tools based on LLM's decision
    4. Feed tool results back to LLM
    5. Repeat until task completed or max iterations reached
    """
    
    def __init__(
        self,
        tools: Optional[List[Union[BaseTool, Type[BaseTool]]]] = None,
        config: Optional[Config] = None,
        memory: Optional[ConversationMemory] = None,
        system_prompt: Optional[str] = None,
    ):
        """
        åˆå§‹åŒ–Agent / Initialize Agent
        
        Args:
            tools: å¯ç”¨å·¥å…·åˆ—è¡¨ï¼ˆå®ä¾‹æˆ–ç±»ï¼‰/ List of available tools (instances or classes)
            config: é…ç½®å¯¹è±¡ / Configuration object
            memory: å¯¹è¯è®°å¿† / Conversation memory
            system_prompt: è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯ / Custom system prompt
        """
        # åˆå§‹åŒ–æ—¥å¿— / Initialize logging
        init_logging()
        
        # åŠ è½½é…ç½® / Load configuration
        self.config = config or get_config()
        
        # åˆå§‹åŒ–å·¥ä½œåŒº / Initialize workspace
        self._init_workspace()
        
        # åˆå§‹åŒ–å·¥å…· / Initialize tools
        self._init_tools(tools)
        
        # åˆå§‹åŒ–è®°å¿† / Initialize memory
        self.memory = memory or get_memory()
        
        # è®¾ç½®ç³»ç»Ÿæç¤ºè¯ / Set system prompt
        if system_prompt:
            self.system_prompt = system_prompt
        else:
            self.system_prompt = get_system_prompt(
                tools=list(self.tools.values()),
                language=self.config.agent.prompt_language
            )
        
        # æ³¨å…¥å·¥ä½œåŒºä¿¡æ¯åˆ°ç³»ç»Ÿæç¤ºè¯ / Inject workspace info into system prompt
        self._inject_workspace_to_prompt()
        
        self.memory.set_system_message(self.system_prompt)
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ / Initialize OpenAI client
        self._init_client()
        
        # AgentçŠ¶æ€ / Agent state
        self.state = AgentState.IDLE
        self.current_step = 0
        self.steps: List[AgentStep] = []
        
        # æ˜¾å¼ä¸Šä¸‹æ–‡æ¨¡å¼ / Explicit context mode
        self.verbose_context = False  # æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¸Šä¸‹æ–‡ / Whether to show detailed context
        
        logger.info(f"Agentåˆå§‹åŒ–å®Œæˆ / Agent initialized with {len(self.tools)} tools")
        logger.info(f"å·¥ä½œåŒº / Workspace: {self.workspace}")
    
    def _init_workspace(self) -> None:
        """
        åˆå§‹åŒ–å·¥ä½œåŒºç›®å½• / Initialize workspace directory
        
        åˆ›å»ºå·¥ä½œåŒºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ï¼Œå¹¶è®¾ç½®å·¥ä½œåŒºè·¯å¾„
        Creates workspace directory if not exists, and sets workspace path
        """
        workspace_path = Path(self.config.agent.workspace)
        workspace_path.mkdir(parents=True, exist_ok=True)
        self.workspace = str(workspace_path.resolve())
        logger.info(f"å·¥ä½œåŒºå·²åˆå§‹åŒ– / Workspace initialized: {self.workspace}")
    
    def _inject_workspace_to_prompt(self) -> None:
        """
        å°†å·¥ä½œåŒºä¿¡æ¯æ³¨å…¥åˆ°ç³»ç»Ÿæç¤ºè¯ / Inject workspace info into system prompt
        
        å‘ŠçŸ¥Agentå¯ä»¥ä½¿ç”¨çš„å·¥ä½œåŒºè·¯å¾„å’Œæ–‡ä»¶ä¸Šä¸‹æ–‡ç®¡ç†èƒ½åŠ›
        Inform Agent about available workspace path and file context management capabilities
        """
        workspace_section = f"""

## å·¥ä½œåŒºä¸æ–‡ä»¶ç®¡ç† / Workspace and File Management

**å·¥ä½œåŒºè·¯å¾„ / Workspace Path**: `{self.workspace}`

ä½ å¯ä»¥åœ¨å·¥ä½œåŒºä¸­åˆ›å»ºã€ä¿®æ”¹å’Œç®¡ç†æ–‡ä»¶ã€‚å¯¹äºé‡è¦çš„ä¸­é—´æ–‡ä»¶ï¼ˆå¦‚ç”Ÿæˆçš„è„šæœ¬ã€æ•°æ®æ–‡ä»¶ç­‰ï¼‰ï¼Œä½ åº”è¯¥ï¼š
You can create, modify and manage files in the workspace. For important intermediate files (e.g., generated scripts, data files), you should:

1. **å°†æ–‡ä»¶ä¿å­˜åˆ°å·¥ä½œåŒº** / Save files to workspace
2. **ä½¿ç”¨æ–‡ä»¶å·¥å…·è®°å½•æ–‡ä»¶ä¸Šä¸‹æ–‡** / Record file context using file tools
3. **åœ¨å¯¹è¯ä¸­å¼•ç”¨è¿™äº›æ–‡ä»¶** / Reference these files in conversation
4. **åŠæ—¶æ¸…ç†ä¸å†éœ€è¦çš„æ—§æ–‡ä»¶** / Clean up old files that are no longer needed

å½“å‰æ–‡ä»¶ä¸Šä¸‹æ–‡ / Current file context:
{self.memory.get_files_summary()}

## å·¥ä½œè®°å¿† / Working Memory

{self.memory.get_context_summary()}

âš ï¸ **é‡è¦æç¤º / Important Notes**:
- é¿å…é‡å¤æ‰§è¡Œç›¸åŒæ“ä½œ / Avoid repeating the same operations
- å¦‚æœæŸä¸ªå·¥å…·å·²ç»æˆåŠŸè°ƒç”¨è¿‡ï¼Œåˆ†æç»“æœè€Œä¸æ˜¯é‡å¤è°ƒç”¨ / If a tool has been called successfully, analyze the result instead of calling again
- æ¯æ¬¡æ“ä½œå‰ï¼Œå…ˆæ£€æŸ¥å·¥ä½œè®°å¿†ä¸­æ˜¯å¦å·²æœ‰ç›¸å…³ç»“æœ / Before each operation, check if results already exist in working memory
"""
        self.system_prompt += workspace_section
    
    def _extract_thinking(self, content: str) -> Optional[str]:
        """
        ä»å†…å®¹ä¸­æå– <thinking> æ ‡ç­¾å†…çš„æ–‡æœ¬ / Extract text within <thinking> tags
        
        Args:
            content: æ¶ˆæ¯å†…å®¹ / Message content
            
        Returns:
            Optional[str]: æ€è€ƒå†…å®¹ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å› None / Thinking content, None if not found
        """
        import re
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå– <thinking>...</thinking> ä¹‹é—´çš„å†…å®¹
        # Use regex to extract content between <thinking>...</thinking>
        pattern = r'<thinking>(.*?)</thinking>'
        match = re.search(pattern, content, re.DOTALL | re.IGNORECASE)
        
        if match:
            thinking_text = match.group(1).strip()
            return thinking_text
        
        return None
    
    def _init_tools(self, tools: Optional[List[Union[BaseTool, Type[BaseTool]]]] = None) -> None:
        """
        åˆå§‹åŒ–å·¥å…· / Initialize tools
        
        Args:
            tools: å·¥å…·åˆ—è¡¨ / List of tools
        """
        self.tools: Dict[str, BaseTool] = {}
        
        # ä½¿ç”¨é»˜è®¤å·¥å…·æˆ–è‡ªå®šä¹‰å·¥å…· / Use default or custom tools
        tool_list = tools or DEFAULT_TOOLS
        
        for tool in tool_list:
            # å¦‚æœæ˜¯ç±»ï¼Œå®ä¾‹åŒ–å®ƒ / If it's a class, instantiate it
            if isinstance(tool, type):
                tool_instance = tool()
            else:
                tool_instance = tool
            
            self.tools[tool_instance.name] = tool_instance
            logger.debug(f"æ³¨å†Œå·¥å…· / Registered tool: {tool_instance.name}")
    
    def _init_client(self) -> None:
        """
        åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ / Initialize OpenAI client
        
        ä½¿ç”¨OpenAIå…¼å®¹æ¥å£è¿æ¥æ¨¡å‹
        Uses OpenAI compatible interface to connect to model
        """
        self.client = OpenAI(
            api_key=self.config.model.api_key,
            base_url=self.config.model.base_url,
            timeout=self.config.model.timeout
        )
        
        logger.info(f"LLMå®¢æˆ·ç«¯åˆå§‹åŒ– / LLM client initialized: {self.config.model.base_url}")
    
    def _get_tools_for_api(self) -> List[Dict[str, Any]]:
        """
        è·å–APIæ ¼å¼çš„å·¥å…·å®šä¹‰ / Get tool definitions in API format
        
        ç¬¦åˆKimi APIè¦æ±‚çš„å·¥å…·å®šä¹‰æ ¼å¼
        Tool definition format compliant with Kimi API requirements
        
        Returns:
            List[Dict]: OpenAIå‡½æ•°è°ƒç”¨æ ¼å¼çš„å·¥å…·å®šä¹‰ / Tool definitions in OpenAI function calling format
        """
        tools = []
        for tool in self.tools.values():
            # è·å–å·¥å…·çš„Schemaå®šä¹‰
            # Get tool schema definition
            tool_schema = tool.get_schema()
            
            # ç¡®ä¿åŸºç¡€ç»“æ„ç¬¦åˆKimiè¦æ±‚
            # Ensure basic structure complies with Kimi requirements
            tool_def = {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description if tool.description else f"{tool.name} tool",
                    "parameters": {
                        "type": "object",
                        "properties": {},
                        "required": []
                    }
                }
            }
            
            # åˆå¹¶Schemaä¸­çš„å‚æ•°å®šä¹‰
            # Merge parameter definitions from schema
            if "function" in tool_schema and "parameters" in tool_schema["function"]:
                params = tool_schema["function"]["parameters"]
                
                # ç¡®ä¿parametersæ˜¯objectç±»å‹
                # Ensure parameters is object type
                if isinstance(params, dict):
                    if "properties" in params:
                        tool_def["function"]["parameters"]["properties"] = params["properties"]
                    if "required" in params:
                        tool_def["function"]["parameters"]["required"] = params["required"]
            
            tools.append(tool_def)
        
        return tools
    
    def _call_llm(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:
        """
        è°ƒç”¨LLM / Call LLM
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ / List of messages
            
        Returns:
            Dict: LLMå“åº” / LLM response
        """
        try:
            response = self.client.chat.completions.create(
                model=self.config.model.effective_model_name,
                messages=messages,
                tools=self._get_tools_for_api() if self.tools else None,
                temperature=self.config.model.temperature,
                max_tokens=self.config.model.max_tokens,
            )
            
            return response
            
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥ / LLM call failed: {str(e)}")
            raise
    
    def _execute_tool(self, tool_name: str, arguments: Dict[str, Any]) -> ToolResult:
        """
        æ‰§è¡Œå·¥å…· / Execute tool
        
        Args:
            tool_name: å·¥å…·åç§° / Tool name
            arguments: å·¥å…·å‚æ•° / Tool arguments
            
        Returns:
            ToolResult: å·¥å…·æ‰§è¡Œç»“æœ / Tool execution result
        """
        if tool_name not in self.tools:
            return ToolResult(
                tool_name=tool_name,
                status=ToolResultStatus.ERROR,
                output=None,
                error_message=f"æœªçŸ¥å·¥å…· / Unknown tool: {tool_name}"
            )
        
        tool = self.tools[tool_name]
        logger.info(f"æ‰§è¡Œå·¥å…· / Executing tool: {tool_name}")
        logger.debug(f"å‚æ•° / Arguments: {arguments}")
        
        result = tool.run(**arguments)
        
        logger.info(f"å·¥å…·æ‰§è¡Œå®Œæˆ / Tool execution completed: {tool_name}, çŠ¶æ€/status: {result.status.value}")
        
        return result
    
    def _process_tool_calls(self, tool_calls: List[Any]) -> List[ToolResult]:
        """
        å¤„ç†å·¥å…·è°ƒç”¨ / Process tool calls
        
        Args:
            tool_calls: å·¥å…·è°ƒç”¨åˆ—è¡¨ / List of tool calls
            
        Returns:
            List[ToolResult]: å·¥å…·æ‰§è¡Œç»“æœåˆ—è¡¨ / List of tool execution results
        """
        results = []
        # ç»´æŠ¤tool_call_idçš„æ˜ å°„
        # Maintain mapping of tool_call_ids
        self._tool_call_ids = {}
        
        for i, tool_call in enumerate(tool_calls):
            tool_name = tool_call.function.name
            
            # è§£æå‚æ•° / Parse arguments
            try:
                arguments = json.loads(tool_call.function.arguments)
            except json.JSONDecodeError:
                arguments = {}
            
            # è·å–tool_call_idï¼ˆä½¿ç”¨LLMè¿”å›çš„ï¼‰
            # Get tool_call_id from LLM response
            call_id = getattr(tool_call, "id", None) or f"call_{tool_name}_{i}"
            self._tool_call_ids[i] = call_id
            
            # è®°å½•å·¥å…·è°ƒç”¨ / Record tool call
            call_record = ToolCall(
                tool_name=tool_name,
                arguments=arguments,
                call_id=call_id
            )
            
            # æ‰§è¡Œå·¥å…· / Execute tool
            result = self._execute_tool(tool_name, arguments)
            results.append(result)
            
            # æ›´æ–°å½“å‰æ­¥éª¤ / Update current step
            if self.steps:
                self.steps[-1].tool_calls.append(call_record)
                self.steps[-1].tool_results.append(result)
        
        return results
    
    def run(self, user_input: str) -> AgentResponse:
        """
        è¿è¡ŒAgentå¤„ç†ç”¨æˆ·è¯·æ±‚ / Run Agent to process user request
        
        è¿™æ˜¯Agentçš„ä¸»å¾ªç¯ï¼Œå®ç°"å·¥å…·å¾ªç¯"æ¨¡å¼
        This is the Agent's main loop, implementing the "tool loop" pattern
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥ / User input
            
        Returns:
            AgentResponse: Agentå“åº” / Agent response
        """
        start_time = time.time()
        request_id = generate_request_id()
        
        logger.info(f"å¼€å§‹å¤„ç†è¯·æ±‚ / Starting request: {request_id}")
        logger.debug(f"ç”¨æˆ·è¾“å…¥ / User input: {user_input}")
        
        # é‡ç½®çŠ¶æ€ / Reset state
        self.state = AgentState.THINKING
        self.current_step = 0
        self.steps = []
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°è®°å¿† / Add user message to memory
        self.memory.add_user_message(user_input)
        
        try:
            # Agentå¾ªç¯ / Agent loop
            while self.current_step < self.config.agent.max_iterations:
                self.current_step += 1
                
                logger.info(f"è¿­ä»£ / Iteration {self.current_step}/{self.config.agent.max_iterations}")
                
                # è·å–å¯¹è¯å†å² / Get conversation history
                messages = self.memory.get_openai_messages()
                
                # å¦‚æœå¼€å¯æ˜¾å¼ä¸Šä¸‹æ–‡æ¨¡å¼ï¼Œæ‰“å°å½“å‰ä¸Šä¸‹æ–‡ / Print context if verbose mode enabled
                if self.verbose_context:
                    self._print_current_context(messages)
                
                # è°ƒç”¨LLM / Call LLM
                self.state = AgentState.THINKING
                response = self._call_llm(messages)
                
                # è§£æå“åº” / Parse response
                choice = response.choices[0]
                message = choice.message
                
                # æå– thinkingï¼ˆExtended Thinkingï¼‰/ Extract thinking
                content = message.content or ""
                thinking_text = self._extract_thinking(content)
                
                # å¦‚æœæ²¡æœ‰ thinking æ ‡ç­¾ï¼Œä½¿ç”¨å†…å®¹å‰500å­—ç¬¦ä½œä¸ºå¤‡ç”¨ / Use first 500 chars as fallback
                if not thinking_text:
                    thinking_text = content[:500] if content else ""
                
                # åˆ›å»ºæ­¥éª¤è®°å½• / Create step record
                step = AgentStep(
                    step_number=self.current_step,
                    thinking=thinking_text
                )
                self.steps.append(step)
                
                # å¦‚æœæœ‰ thinkingï¼Œè®°å½•åˆ°æ—¥å¿— / Log thinking if present
                if thinking_text:
                    logger.info(f"ğŸ’­ Thinking: {thinking_text[:200]}..." if len(thinking_text) > 200 else f"ğŸ’­ Thinking: {thinking_text}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨ / Check for tool calls
                if message.tool_calls:
                    self.state = AgentState.EXECUTING
                    
                    # å¤„ç†å·¥å…·è°ƒç”¨ / Process tool calls
                    tool_results = self._process_tool_calls(message.tool_calls)
                    
                    # å°†åŠ©æ‰‹æ¶ˆæ¯æ·»åŠ åˆ°è®°å¿†ï¼ˆåŒ…å«å·¥å…·è°ƒç”¨ï¼‰
                    # Add assistant message to memory (including tool calls)
                    # éœ€è¦å°†tool_callsä¹Ÿæ·»åŠ åˆ°æ¶ˆæ¯ä¸­ï¼Œä»¥ä¾¿Kimi APIèƒ½è¯†åˆ«
                    # Must include tool_calls so Kimi API can recognize them
                    tool_calls_data = []
                    for tool_call in message.tool_calls:
                        tool_calls_data.append({
                            "id": str(tool_call.id) if hasattr(tool_call, "id") else "",
                            "type": "function",
                            "function": {
                                "name": tool_call.function.name,
                                "arguments": tool_call.function.arguments
                            }
                        })
                    
                    self.memory.add_assistant_message(
                        message.content or "",
                        metadata={"tool_calls": tool_calls_data} if tool_calls_data else None
                    )
                    
                    # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°è®°å¿† / Add tool results to memory
                    for i, tool_call in enumerate(message.tool_calls):
                        result = tool_results[i]
                        result_text = format_tool_result(result.output)
                        if result.error_message:
                            result_text = f"é”™è¯¯ / Error: {result.error_message}"

                        # æ·»åŠ å·¥å…·å“åº”æ¶ˆæ¯ï¼Œå¿…é¡»åŒ…å«tool_call_idï¼ˆKimi API è¦æ±‚ï¼‰
                        # Add tool response message with required tool_call_id (Kimi API requirement)
                        metadata = {"tool_name": tool_call.function.name}
                        
                        # ç¡®ä¿æ€»æ˜¯æœ‰tool_call_idï¼ˆKimi APIå¼ºåˆ¶è¦æ±‚ï¼‰
                        # Always ensure tool_call_id is present (Kimi API requirement)
                        call_id = None
                        
                        # é¦–å…ˆå°è¯•ä½¿ç”¨æˆ‘ä»¬ç»´æŠ¤çš„tool_call_idæ˜ å°„
                        # First try to use our maintained tool_call_id mapping
                        if hasattr(self, "_tool_call_ids") and i in self._tool_call_ids:
                            call_id = self._tool_call_ids[i]
                        # å…¶æ¬¡å°è¯•ä½¿ç”¨åŸå§‹å·¥å…·è°ƒç”¨ID
                        # Otherwise use original tool call ID
                        elif hasattr(tool_call, "id") and tool_call.id:
                            call_id = str(tool_call.id).strip()
                        # æœ€åç”Ÿæˆä¸€ä¸ªå¤‡é€‰IDï¼ˆä»¥é˜²ä¸‡ä¸€ï¼‰
                        # Last resort: generate a fallback ID
                        else:
                            call_id = f"call_{tool_call.function.name}_{i}"
                        
                        # æ·»åŠ åˆ°å…ƒæ•°æ®ï¼ˆç¡®ä¿ä¸ä¸ºç©ºï¼‰
                        # Add to metadata (ensure not empty)
                        if call_id:
                            metadata["tool_call_id"] = call_id

                        self.memory.add_tool_message(
                            content=result_text,
                            tool_name=tool_call.function.name,
                            metadata=metadata
                        )
                else:
                    # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œä»»åŠ¡å®Œæˆ / No tool calls, task completed
                    self.state = AgentState.COMPLETED
                    
                    # æ·»åŠ æœ€ç»ˆå“åº”åˆ°è®°å¿† / Add final response to memory
                    final_answer = message.content or ""
                    self.memory.add_assistant_message(final_answer)
                    step.response = final_answer
                    
                    execution_time = time.time() - start_time
                    
                    logger.info(f"ä»»åŠ¡å®Œæˆ / Task completed in {execution_time:.2f}s, {self.current_step} iterations")
                    
                    return AgentResponse(
                        success=True,
                        final_answer=final_answer,
                        steps=self.steps,
                        total_iterations=self.current_step,
                        total_tokens_used=0,
                        execution_time=execution_time
                    )
            
            # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° / Reached max iterations
            self.state = AgentState.STOPPED
            execution_time = time.time() - start_time
            
            logger.warning(f"è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° / Reached max iterations: {self.config.agent.max_iterations}")
            
            return AgentResponse(
                success=False,
                final_answer="è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œä»»åŠ¡æœªå®Œæˆã€‚/ Reached max iterations, task not completed.",
                steps=self.steps,
                total_iterations=self.current_step,
                total_tokens_used=0,
                execution_time=execution_time,
                error_message="Max iterations reached"
            )
            
        except Exception as e:
            self.state = AgentState.ERROR
            execution_time = time.time() - start_time
            
            logger.error(f"Agentæ‰§è¡Œå‡ºé”™ / Agent execution error: {str(e)}")
            
            return AgentResponse(
                success=False,
                final_answer=f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ / Error during execution: {str(e)}",
                steps=self.steps,
                total_iterations=self.current_step,
                total_tokens_used=0,
                execution_time=execution_time,
                error_message=str(e)
            )
    
    async def arun(self, user_input: str) -> AgentResponse:
        """
        å¼‚æ­¥è¿è¡ŒAgent / Run Agent asynchronously
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥ / User input
            
        Returns:
            AgentResponse: Agentå“åº” / Agent response
        """
        # ç›®å‰ç®€å•åœ°è°ƒç”¨åŒæ­¥æ–¹æ³•ï¼Œæœªæ¥å¯ä»¥å®ç°çœŸæ­£çš„å¼‚æ­¥
        # Currently just calls sync method, can implement true async in the future
        return self.run(user_input)
    
    def add_tool(self, tool: Union[BaseTool, Type[BaseTool]]) -> None:
        """
        æ·»åŠ å·¥å…· / Add tool
        
        Args:
            tool: å·¥å…·å®ä¾‹æˆ–ç±» / Tool instance or class
        """
        if isinstance(tool, type):
            tool_instance = tool()
        else:
            tool_instance = tool
        
        self.tools[tool_instance.name] = tool_instance
        logger.info(f"æ·»åŠ å·¥å…· / Added tool: {tool_instance.name}")
        
        # æ›´æ–°ç³»ç»Ÿæç¤ºè¯ / Update system prompt
        self.system_prompt = get_system_prompt(
            tools=list(self.tools.values()),
            language=self.config.agent.prompt_language
        )
        self.memory.set_system_message(self.system_prompt)
    
    def remove_tool(self, tool_name: str) -> bool:
        """
        ç§»é™¤å·¥å…· / Remove tool
        
        Args:
            tool_name: å·¥å…·åç§° / Tool name
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸç§»é™¤ / Whether removal was successful
        """
        if tool_name in self.tools:
            del self.tools[tool_name]
            logger.info(f"ç§»é™¤å·¥å…· / Removed tool: {tool_name}")
            
            # æ›´æ–°ç³»ç»Ÿæç¤ºè¯ / Update system prompt
            self.system_prompt = get_system_prompt(
                tools=list(self.tools.values()),
                language=self.config.agent.prompt_language
            )
            self.memory.set_system_message(self.system_prompt)
            return True
        
        return False
    
    def reset(self) -> None:
        """é‡ç½®AgentçŠ¶æ€ / Reset Agent state"""
        self.state = AgentState.IDLE
        self.current_step = 0
        self.steps = []
        self.memory.clear(keep_system=True)
        
        logger.info("AgentçŠ¶æ€å·²é‡ç½® / Agent state reset")
    
    def get_status(self) -> Dict[str, Any]:
        """
        è·å–AgentçŠ¶æ€ / Get Agent status
        
        Returns:
            Dict: çŠ¶æ€ä¿¡æ¯ / Status information
        """
        return {
            "state": self.state.value,
            "current_step": self.current_step,
            "total_steps": len(self.steps),
            "tools_count": len(self.tools),
            "memory_entries": len(self.memory.messages),
            "context_length": self.memory.get_context_length(),
            "workspace": self.workspace,
            "files_count": len(self.memory.files),
        }
    
    # ==============================================================================
    # æ–‡ä»¶ä¸Šä¸‹æ–‡ç®¡ç†æ–¹æ³• / File Context Management Methods
    # ==============================================================================
    
    def add_file_to_context(
        self,
        path: str,
        content: Optional[str] = None,
        abstract: str = "",
        metadata: Optional[Dict[str, Any]] = None
    ) -> None:
        """
        å°†æ–‡ä»¶æ·»åŠ åˆ°å¯¹è¯ä¸Šä¸‹æ–‡ / Add file to conversation context
        
        Args:
            path: æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹æˆ–ç»å¯¹ï¼‰/ File path (relative or absolute)
            content: æ–‡ä»¶å†…å®¹ / File content
            abstract: æ–‡ä»¶æ‘˜è¦ / File abstract
            metadata: é¢å¤–å…ƒæ•°æ® / Extra metadata
        """
        self.memory.add_file(path, content, abstract, metadata)
        logger.info(f"æ–‡ä»¶å·²æ·»åŠ åˆ°ä¸Šä¸‹æ–‡ / File added to context: {path}")
    
    def update_file_in_context(
        self,
        path: str,
        content: Optional[str] = None,
        abstract: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> bool:
        """
        æ›´æ–°ä¸Šä¸‹æ–‡ä¸­çš„æ–‡ä»¶ / Update file in context
        
        Args:
            path: æ–‡ä»¶è·¯å¾„ / File path
            content: æ–°å†…å®¹ / New content
            abstract: æ–°æ‘˜è¦ / New abstract
            metadata: æ–°å…ƒæ•°æ® / New metadata
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸæ›´æ–° / Whether update was successful
        """
        result = self.memory.update_file(path, content, abstract, metadata)
        if result:
            logger.info(f"æ–‡ä»¶å·²æ›´æ–° / File updated: {path}")
            return True
        else:
            logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•æ›´æ–° / File does not exist, cannot update: {path}")
            return False
    
    def remove_file_from_context(self, path: str) -> bool:
        """
        ä»ä¸Šä¸‹æ–‡ä¸­ç§»é™¤æ–‡ä»¶ / Remove file from context
        
        Args:
            path: æ–‡ä»¶è·¯å¾„ / File path
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸç§»é™¤ / Whether removal was successful
        """
        result = self.memory.remove_file(path)
        if result:
            logger.info(f"æ–‡ä»¶å·²ä»ä¸Šä¸‹æ–‡ç§»é™¤ / File removed from context: {path}")
            return True
        else:
            logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•ç§»é™¤ / File does not exist, cannot remove: {path}")
            return False
    
    def list_context_files(self) -> List[str]:
        """
        åˆ—å‡ºä¸Šä¸‹æ–‡ä¸­çš„æ‰€æœ‰æ–‡ä»¶è·¯å¾„ / List all file paths in context
        
        Returns:
            List[str]: æ–‡ä»¶è·¯å¾„åˆ—è¡¨ / List of file paths
        """
        return [f.path for f in self.memory.list_files()]
    
    def get_files_summary(self) -> str:
        """
        è·å–æ–‡ä»¶ä¸Šä¸‹æ–‡æ‘˜è¦ / Get files context summary
        
        Returns:
            str: æ–‡ä»¶æ‘˜è¦ / Files summary
        """
        return self.memory.get_files_summary()
    
    # ==============================================================================
    # æ˜¾å¼ä¸Šä¸‹æ–‡æ¨¡å¼æ–¹æ³• / Verbose Context Mode Methods
    # ==============================================================================
    
    def toggle_verbose_context(self) -> bool:
        """
        åˆ‡æ¢æ˜¾å¼ä¸Šä¸‹æ–‡æ¨¡å¼ / Toggle verbose context mode
        
        Returns:
            bool: å½“å‰çŠ¶æ€ / Current state
        """
        self.verbose_context = not self.verbose_context
        logger.info(f"æ˜¾å¼ä¸Šä¸‹æ–‡æ¨¡å¼ / Verbose context mode: {'å¼€å¯ / ON' if self.verbose_context else 'å…³é—­ / OFF'}")
        return self.verbose_context
    
    def _print_current_context(self, messages: List[Dict]) -> None:
        """
        æ‰“å°å½“å‰ä¼ å…¥LLMçš„ä¸Šä¸‹æ–‡ / Print current context sent to LLM
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ / Message list
        """
        print("\n" + "="*80)
        print("ğŸ“‹ å½“å‰ä¸Šä¸‹æ–‡ / Current Context")
        print("="*80)
        
        for i, msg in enumerate(messages, 1):
            role = msg.get("role", "unknown")
            content = msg.get("content", "")
            
            # æ ¹æ®è§’è‰²ä½¿ç”¨ä¸åŒçš„å›¾æ ‡å’Œé¢œè‰²æ ‡è¯†
            role_icon = {
                "system": "âš™ï¸",
                "user": "ğŸ‘¤",
                "assistant": "ğŸ¤–",
                "tool": "ğŸ”§"
            }.get(role, "â“")
            
            print(f"\n{role_icon} [{i}] Role: {role}")
            
            # æ˜¾ç¤ºå†…å®¹é¢„è§ˆï¼ˆé™åˆ¶é•¿åº¦ï¼‰
            if len(content) > 300:
                print(f"Content: {content[:300]}...")
                print(f"[... çœç•¥ {len(content) - 300} å­—ç¬¦ / {len(content) - 300} chars omitted]")
            else:
                print(f"Content: {content}")
            
            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œæ˜¾ç¤ºå·¥å…·ä¿¡æ¯
            if "tool_calls" in msg:
                print(f"Tool Calls: {len(msg['tool_calls'])} ä¸ªå·¥å…·è°ƒç”¨")
            
            # å¦‚æœæ˜¯å·¥å…·æ¶ˆæ¯ï¼Œæ˜¾ç¤ºå·¥å…·åç§°
            if role == "tool" and "name" in msg:
                print(f"Tool Name: {msg['name']}")
            
            print("-" * 80)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_chars = sum(len(msg.get("content", "")) for msg in messages)
        print(f"\nğŸ“Š ç»Ÿè®¡ / Statistics:")
        print(f"   - æ¶ˆæ¯æ•°é‡ / Message count: {len(messages)}")
        print(f"   - æ€»å­—ç¬¦æ•° / Total characters: {total_chars:,}")
        print(f"   - ä¼°è®¡tokenæ•° / Estimated tokens: ~{total_chars // 4:,}")
        print("="*80 + "\n")
