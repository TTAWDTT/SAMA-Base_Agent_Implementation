#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ==============================================================================
# GAIA Benchmark æµ‹è¯•å¯åŠ¨å™¨ / GAIA Benchmark Test Launcher
# ==============================================================================
# ä½¿ç”¨æ–¹æ³• / Usage:
#   python launch.py                      # å¤„ç†é»˜è®¤è¡Œï¼ˆç¬¬1è¡Œï¼‰
#   python launch.py --row 5              # å¤„ç†ç¬¬5è¡Œ
#   python launch.py --row 1-10           # å¤„ç†ç¬¬1åˆ°10è¡Œ
#   python launch.py --row 1,3,5          # å¤„ç†ç¬¬1ã€3ã€5è¡Œ
#   python launch.py --list               # åˆ—å‡ºæ•°æ®é›†å†…å®¹
#   python launch.py --help               # æ˜¾ç¤ºå¸®åŠ©
# ==============================================================================

import argparse
import json
import os
import shutil
import sys
import io
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

# ä¿®å¤ Windows ç¼–ç é—®é¢˜ / Fix Windows encoding issues
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ / Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src import (
    BaseAgent,
    get_config,
    init_logging,
    get_logger,
    DocumentConverter,
    preprocess_files,
)


# ==============================================================================
# å¸¸é‡å®šä¹‰ / Constants
# ==============================================================================

# é»˜è®¤æ•°æ®é›†è·¯å¾„ / Default dataset path
DEFAULT_DATASET_PATH = "dataset/data/train-00000-of-00001.parquet"

# è¾“å‡ºç›®å½• / Output directory
OUTPUT_DIR = "outputs"


# ==============================================================================
# è¾…åŠ©å‡½æ•° / Helper Functions
# ==============================================================================

def load_dataset(dataset_path: str) -> Optional[Any]:
    """
    åŠ è½½æ•°æ®é›† / Load dataset
    
    Args:
        dataset_path: æ•°æ®é›†è·¯å¾„ / Dataset path
        
    Returns:
        DataFrame æˆ– None / DataFrame or None
    """
    try:
        import pandas as pd
        
        if not os.path.exists(dataset_path):
            print(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨ / Dataset file not found: {dataset_path}")
            return None
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©è¯»å–æ–¹å¼ / Choose reading method by file type
        ext = Path(dataset_path).suffix.lower()
        
        if ext == '.parquet':
            try:
                df = pd.read_parquet(dataset_path, engine='fastparquet')
            except ImportError:
                df = pd.read_parquet(dataset_path, engine='pyarrow')
        elif ext == '.csv':
            df = pd.read_csv(dataset_path)
        elif ext == '.json':
            df = pd.read_json(dataset_path)
        else:
            print(f"âŒ ä¸æ”¯æŒçš„æ•°æ®é›†æ ¼å¼ / Unsupported dataset format: {ext}")
            return None
        
        return df
        
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ– / Missing dependency: {e}")
        print("   è¯·è¿è¡Œ / Please run: pip install pandas fastparquet pyarrow")
        return None
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥ / Failed to load dataset: {e}")
        return None


def parse_row_indices(row_spec: str, max_rows: int) -> List[int]:
    """
    è§£æè¡Œç´¢å¼•è§„èŒƒ / Parse row index specification
    
    æ”¯æŒæ ¼å¼ / Supported formats:
    - å•ä¸ªæ•°å­—: "5"
    - èŒƒå›´: "1-10"
    - åˆ—è¡¨: "1,3,5,7"
    - æ··åˆ: "1-3,5,7-9"
    
    Args:
        row_spec: è¡Œè§„èŒƒå­—ç¬¦ä¸² / Row specification string
        max_rows: æœ€å¤§è¡Œæ•° / Maximum rows
        
    Returns:
        List[int]: è¡Œç´¢å¼•åˆ—è¡¨ / List of row indices
    """
    indices = []
    
    for part in row_spec.split(','):
        part = part.strip()
        
        if '-' in part:
            # èŒƒå›´ / Range
            try:
                start, end = map(int, part.split('-'))
                start = max(0, start)
                end = min(max_rows - 1, end)
                indices.extend(range(start, end + 1))
            except ValueError:
                print(f"âš ï¸  æ— æ•ˆçš„èŒƒå›´æ ¼å¼ / Invalid range format: {part}")
        else:
            # å•ä¸ªæ•°å­— / Single number
            try:
                idx = int(part)
                if 0 <= idx < max_rows:
                    indices.append(idx)
                else:
                    print(f"âš ï¸  è¡Œç´¢å¼•è¶…å‡ºèŒƒå›´ / Row index out of range: {idx} (max: {max_rows - 1})")
            except ValueError:
                print(f"âš ï¸  æ— æ•ˆçš„è¡Œå· / Invalid row number: {part}")
    
    # å»é‡å¹¶æ’åº / Deduplicate and sort
    return sorted(set(indices))


def extract_task_info(row: Any) -> Tuple[str, str, List[str]]:
    """
    ä»æ•°æ®è¡Œæå–ä»»åŠ¡ä¿¡æ¯ / Extract task info from data row
    
    Args:
        row: æ•°æ®è¡Œ / Data row
        
    Returns:
        Tuple[str, str, List[str]]: (task_id, prompt, reference_files)
    """
    # è·å–ä»»åŠ¡ID / Get task ID
    task_id = str(row.get('task_id', row.name if hasattr(row, 'name') else 'unknown'))
    
    # è·å–æç¤ºè¯ / Get prompt
    prompt = str(row.get('prompt', row.get('question', row.get('input', ''))))
    
    # è·å–å‚è€ƒæ–‡ä»¶ / Get reference files
    ref_files_raw = row.get('reference_files', row.get('files', row.get('attachments', [])))
    
    # å¤„ç†å‚è€ƒæ–‡ä»¶è·¯å¾„ / Process reference file paths
    reference_files = []
    if isinstance(ref_files_raw, (list, tuple)):
        for file in ref_files_raw:
            if file:  # è¿‡æ»¤ç©ºå€¼ / Filter empty values
                file_path = str(file)
                # æ·»åŠ  dataset/ å‰ç¼€ï¼ˆå¦‚æœä¸æ˜¯ç»å¯¹è·¯å¾„ï¼‰/ Add dataset/ prefix if not absolute
                if not os.path.isabs(file_path) and not file_path.startswith('dataset/'):
                    file_path = f"dataset/{file_path}"
                reference_files.append(file_path)
    elif ref_files_raw and str(ref_files_raw).strip():
        file_path = str(ref_files_raw)
        if not os.path.isabs(file_path) and not file_path.startswith('dataset/'):
            file_path = f"dataset/{file_path}"
        reference_files.append(file_path)
    
    return task_id, prompt, reference_files


def save_result(
    task_id: str,
    prompt: str,
    response: Any,
    processed_files: Optional[Dict] = None
) -> str:
    """
    ä¿å­˜ä»»åŠ¡ç»“æœ / Save task result
    
    Args:
        task_id: ä»»åŠ¡ID / Task ID
        prompt: åŸå§‹æç¤ºè¯ / Original prompt
        response: Agentå“åº” / Agent response
        processed_files: é¢„å¤„ç†çš„æ–‡ä»¶ä¿¡æ¯ / Preprocessed file info
        
    Returns:
        str: è¾“å‡ºç›®å½•è·¯å¾„ / Output directory path
    """
    # åˆ›å»ºè¾“å‡ºç›®å½• / Create output directory
    output_dir = Path(OUTPUT_DIR) / task_id
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æ„å»ºç»“æœæ•°æ® / Build result data
    result = {
        "task_id": task_id,
        "timestamp": datetime.now().isoformat(),
        "prompt": prompt,
        "success": response.success if hasattr(response, 'success') else False,
        "final_answer": response.final_answer if hasattr(response, 'final_answer') else str(response),
        "total_iterations": response.total_iterations if hasattr(response, 'total_iterations') else 0,
        "execution_time": response.execution_time if hasattr(response, 'execution_time') else 0,
        "error_message": response.error_message if hasattr(response, 'error_message') else None,
    }
    
    # æ·»åŠ é¢„å¤„ç†æ–‡ä»¶ä¿¡æ¯ / Add preprocessed file info
    if processed_files:
        result["processed_files"] = {
            "file_count": processed_files.get("file_count", 0),
            "image_count": processed_files.get("image_count", 0),
            "files": processed_files.get("files", []),
        }
    
    # ä¿å­˜ç»“æœJSON / Save result JSON
    result_path = output_dir / "result.json"
    with open(result_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜æœ€ç»ˆç­”æ¡ˆåˆ°æ–‡æœ¬æ–‡ä»¶ / Save final answer to text file
    answer_path = output_dir / "answer.txt"
    with open(answer_path, 'w', encoding='utf-8') as f:
        f.write(result["final_answer"])
    
    print(f"âœ… ç»“æœå·²ä¿å­˜ / Result saved to: {output_dir}")
    
    return str(output_dir)


def _snapshot_workspace_files(root: Path) -> set:
    """
    å¿«ç…§å·¥ä½œåŒºæ ¹ç›®å½• workspace/ ä¸‹çš„é¡¶å±‚æ–‡ä»¶åï¼ˆä¸é€’å½’å­ç›®å½•ï¼‰ã€‚
    è¿”å›æ–‡ä»¶åé›†åˆï¼ˆåªåŒ…å«é¡¶å±‚æ–‡ä»¶ï¼Œä¸å«è·¯å¾„ï¼‰ã€‚
    """
    files = set()
    ws = Path(root) / "workspace"
    try:
        if not ws.exists() or not ws.is_dir():
            return files
        for p in ws.iterdir():
            try:
                if p.is_file():
                    files.add(p.name)
            except Exception:
                continue
    except Exception:
        return set()
    return files


def _move_workspace_files_to_output(new_files: set, task_id: str, root: Path) -> None:
    """
    å°† workspace/ æ ¹ç›®å½•ä¸‹çš„æ–°é¡¶å±‚æ–‡ä»¶ç§»åŠ¨åˆ° outputs/{task_id} ä¸‹ï¼Œä¿ç•™æ–‡ä»¶åã€‚
    åªå¤„ç†æ–‡ä»¶ï¼Œä¸å¤„ç†æ–‡ä»¶å¤¹æˆ–å­ç›®å½•å†…çš„å†…å®¹ã€‚
    """
    ws = Path(root) / "workspace"
    out_base = Path(root) / OUTPUT_DIR / task_id
    out_base.mkdir(parents=True, exist_ok=True)

    for name in sorted(new_files):
        src = ws / name
        if not src.exists() or not src.is_file():
            continue
        dest = out_base / name
        try:
            shutil.move(str(src), str(dest))
            print(f"   â†ªï¸  å·²ç§»åŠ¨ workspace/{name} -> {dest}")
        except Exception as e:
            print(f"   âš ï¸  æ— æ³•ç§»åŠ¨ workspace/{name}: {e}")


# ==============================================================================
# ä¸»å¤„ç†å‡½æ•° / Main Processing Functions
# ==============================================================================

def process_task(
    task_id: str,
    prompt: str,
    reference_files: List[str],
    agent: BaseAgent,
    logger: Any
) -> Any:
    """
    å¤„ç†å•ä¸ªä»»åŠ¡ / Process single task
    
    Args:
        task_id: ä»»åŠ¡ID / Task ID
        prompt: æç¤ºè¯ / Prompt
        reference_files: å‚è€ƒæ–‡ä»¶åˆ—è¡¨ / Reference file list
        agent: Agentå®ä¾‹ / Agent instance
        logger: æ—¥å¿—å™¨ / Logger
        
    Returns:
        Agentå“åº” / Agent response
    """
    print(f"\n{'=' * 60}")
    print(f"ğŸ“‹ ä»»åŠ¡ / Task: {task_id}")
    print(f"{'=' * 60}")
    
    # é¢„å¤„ç†æ–‡ä»¶ / Preprocess files
    processed_files = None
    enhanced_prompt = prompt
    
    if reference_files:
        print(f"ğŸ“ å‘ç° {len(reference_files)} ä¸ªå‚è€ƒæ–‡ä»¶ / Found {len(reference_files)} reference files")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ / Check if files exist
        existing_files = []
        for file_path in reference_files:
            if os.path.exists(file_path):
                existing_files.append(file_path)
                print(f"   âœ“ {file_path}")
            else:
                print(f"   âœ— {file_path} (ä¸å­˜åœ¨ / not found)")
        
        # å¤„ç†å­˜åœ¨çš„æ–‡ä»¶ / Process existing files
        if existing_files:
            try:
                print(f"\nğŸ”„ æ­£åœ¨é¢„å¤„ç†æ–‡ä»¶... / Preprocessing files...")
                processed_files = preprocess_files(task_id, existing_files)
                
                # å¢å¼ºæç¤ºè¯ / Enhance prompt
                file_content = processed_files.get("content", "")
                if file_content:
                    enhanced_prompt = f"""{prompt}

## å‚è€ƒæ–‡ä»¶å†…å®¹ / Reference File Content

{file_content}
"""
                    print(f"   ğŸ“„ å·²å¤„ç† {processed_files.get('file_count', 0)} ä¸ªæ–‡æ¡£")
                    print(f"   ğŸ–¼ï¸  å·²å¤„ç† {processed_files.get('image_count', 0)} å¼ å›¾ç‰‡")
                
            except Exception as e:
                logger.error(f"æ–‡ä»¶é¢„å¤„ç†å¤±è´¥ / File preprocessing failed: {e}")
                print(f"âš ï¸  æ–‡ä»¶é¢„å¤„ç†å¤±è´¥ / File preprocessing failed: {e}")
    
    # æ˜¾ç¤ºæç¤ºè¯ / Display prompt
    print(f"\nğŸ“ æç¤ºè¯ / Prompt:")
    print(f"   {prompt[:200]}..." if len(prompt) > 200 else f"   {prompt}")
    
    # è¿è¡ŒAgent / Run Agent
    print(f"\nğŸ¤– Agentå¼€å§‹å¤„ç†... / Agent processing...")
    
    try:
        response = agent.run(enhanced_prompt)
        
        # æ˜¾ç¤ºç»“æœ / Display result
        print(f"\n{'=' * 60}")
        print("ğŸ“Š å¤„ç†ç»“æœ / Result:")
        print(f"{'=' * 60}")
        print(f"âœ… æˆåŠŸ / Success: {response.success}")
        print(f"ğŸ”„ è¿­ä»£æ¬¡æ•° / Iterations: {response.total_iterations}")
        print(f"â±ï¸  è€—æ—¶ / Time: {response.execution_time:.2f}s")
        print(f"\nğŸ’¬ æœ€ç»ˆç­”æ¡ˆ / Final Answer:")
        print(f"   {response.final_answer[:500]}..." if len(response.final_answer) > 500 else f"   {response.final_answer}")
        
        # ä¿å­˜ç»“æœ / Save result
        save_result(task_id, prompt, response, processed_files)
        
        return response
        
    except Exception as e:
        logger.error(f"Agentæ‰§è¡Œå¤±è´¥ / Agent execution failed: {e}")
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥ / Execution failed: {e}")
        raise


def list_dataset(df: Any, start: int = 0, count: int = 10) -> None:
    """
    åˆ—å‡ºæ•°æ®é›†å†…å®¹ / List dataset content
    
    Args:
        df: DataFrame
        start: èµ·å§‹è¡Œ / Start row
        count: æ˜¾ç¤ºæ•°é‡ / Display count
    """
    print(f"\nğŸ“‹ æ•°æ®é›†ä¿¡æ¯ / Dataset Info")
    print(f"{'=' * 60}")
    print(f"æ€»è¡Œæ•° / Total rows: {len(df)}")
    print(f"åˆ—å / Columns: {list(df.columns)}")
    print(f"\n{'=' * 60}")
    print(f"æ•°æ®é¢„è§ˆ / Data Preview (rows {start} - {min(start + count, len(df))})")
    print(f"{'=' * 60}\n")
    
    for idx in range(start, min(start + count, len(df))):
        row = df.iloc[idx]
        task_id, prompt, ref_files = extract_task_info(row)
        
        print(f"[{idx}] Task: {task_id}")
        print(f"    Prompt: {prompt[:80]}..." if len(prompt) > 80 else f"    Prompt: {prompt}")
        print(f"    Files: {len(ref_files)} ä¸ª")
        print()


# ==============================================================================
# ä¸»å‡½æ•° / Main Function
# ==============================================================================

def main():
    """
    ä¸»å‡½æ•° / Main function
    """
    # è§£æå‘½ä»¤è¡Œå‚æ•° / Parse command line arguments
    parser = argparse.ArgumentParser(
        description="GAIA Benchmark æµ‹è¯•å¯åŠ¨å™¨ / GAIA Benchmark Test Launcher",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ / Examples:
  python launch.py                      # å¤„ç†ç¬¬0è¡Œ / Process row 0
  python launch.py --row 5              # å¤„ç†ç¬¬5è¡Œ / Process row 5
  python launch.py --row 1-10           # å¤„ç†ç¬¬1åˆ°10è¡Œ / Process rows 1-10
  python launch.py --row 1,3,5          # å¤„ç†æŒ‡å®šè¡Œ / Process specified rows
  python launch.py --list               # åˆ—å‡ºæ•°æ®é›† / List dataset
  python launch.py --list --start 10    # ä»ç¬¬10è¡Œå¼€å§‹åˆ—å‡º / List from row 10

æ›´å¤šä¿¡æ¯è¯·å‚é˜… GAIA_Benchmark_Preparation_Guide.md
For more information, see GAIA_Benchmark_Preparation_Guide.md
        """
    )
    
    parser.add_argument(
        "-r", "--row",
        type=str,
        default="0",
        help="è¦å¤„ç†çš„è¡Œå·ï¼ˆæ”¯æŒèŒƒå›´å’Œåˆ—è¡¨ï¼‰/ Row to process (supports range and list)"
    )
    
    parser.add_argument(
        "-d", "--dataset",
        type=str,
        default=DEFAULT_DATASET_PATH,
        help="æ•°æ®é›†è·¯å¾„ / Dataset path"
    )
    
    parser.add_argument(
        "-l", "--list",
        action="store_true",
        help="åˆ—å‡ºæ•°æ®é›†å†…å®¹ / List dataset content"
    )
    
    parser.add_argument(
        "-s", "--start",
        type=int,
        default=0,
        help="åˆ—è¡¨èµ·å§‹è¡Œï¼ˆä¸--listä¸€èµ·ä½¿ç”¨ï¼‰/ List start row (use with --list)"
    )
    
    parser.add_argument(
        "-n", "--count",
        type=int,
        default=10,
        help="åˆ—è¡¨æ˜¾ç¤ºæ•°é‡ï¼ˆä¸--listä¸€èµ·ä½¿ç”¨ï¼‰/ List display count (use with --list)"
    )
    
    parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="å¯ç”¨è¯¦ç»†è¾“å‡º / Enable verbose output"
    )
    
    parser.add_argument(
        "--version",
        action="version",
        version="GAIA Launcher v0.1.0"
    )
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ—¥å¿— / Initialize logging
    init_logging()
    logger = get_logger("launch")
    
    # åŠ è½½æ•°æ®é›† / Load dataset
    print(f"\nğŸ“‚ åŠ è½½æ•°æ®é›† / Loading dataset: {args.dataset}")
    df = load_dataset(args.dataset)
    
    if df is None:
        sys.exit(1)
    
    print(f"   âœ“ åŠ è½½æˆåŠŸ / Loaded successfully: {len(df)} è¡Œ / rows")
    
    # åˆ—è¡¨æ¨¡å¼ / List mode
    if args.list:
        list_dataset(df, args.start, args.count)
        return
    
    # è§£æè¡Œç´¢å¼• / Parse row indices
    row_indices = parse_row_indices(args.row, len(df))
    
    if not row_indices:
        print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¡Œç´¢å¼• / No valid row indices found")
        sys.exit(1)
    
    print(f"ğŸ“Œ å°†å¤„ç† {len(row_indices)} ä¸ªä»»åŠ¡ / Will process {len(row_indices)} tasks")
    print(f"   è¡Œå· / Rows: {row_indices}")
    
    # åˆ›å»ºAgent / Create Agent
    print(f"\nğŸ¤– åˆå§‹åŒ–Agent / Initializing Agent...")
    
    try:
        agent = BaseAgent()
        print("   âœ“ Agentåˆå§‹åŒ–æˆåŠŸ / Agent initialized successfully")
    except Exception as e:
        print(f"âŒ Agentåˆå§‹åŒ–å¤±è´¥ / Agent initialization failed: {e}")
        sys.exit(1)
    
    # å¤„ç†ä»»åŠ¡ / Process tasks
    results = []
    
    for idx in row_indices:
        try:
            # å¿«ç…§ workspace/ æ ¹ç›®å½•åœ¨ä»»åŠ¡å¼€å§‹å‰çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆä»…é¡¶å±‚æ–‡ä»¶ï¼‰
            _before_snapshot = _snapshot_workspace_files(project_root)

            row = df.iloc[idx]
            task_id, prompt, reference_files = extract_task_info(row)

            response = process_task(task_id, prompt, reference_files, agent, logger)
            results.append({
                "row": idx,
                "task_id": task_id,
                "success": response.success,
            })

            # å¿«ç…§ä»»åŠ¡ç»“æŸå workspace/ æ ¹ç›®å½•æ–‡ä»¶åˆ—è¡¨ï¼Œç§»åŠ¨æ–°å¢çš„é¡¶å±‚æ–‡ä»¶åˆ° outputs/{task_id}
            _after_snapshot = _snapshot_workspace_files(project_root)
            new_files = set(_after_snapshot) - set(_before_snapshot)
            if new_files:
                print(f"\nğŸ“¦ å‘ç° {len(new_files)} ä¸ªæ–°æ–‡ä»¶åœ¨ workspace/ æ ¹ç›®å½•ï¼Œå°†ç§»åŠ¨åˆ° outputs/{task_id}...")
                _move_workspace_files_to_output(new_files, task_id, project_root)

            # é‡ç½®AgentçŠ¶æ€ / Reset Agent state
            agent.reset()

        except Exception as e:
            logger.error(f"ä»»åŠ¡å¤„ç†å¤±è´¥ / Task processing failed: row={idx}, error={e}")
            results.append({
                "row": idx,
                "task_id": task_id if 'task_id' in locals() else 'unknown',
                "success": False,
                "error": str(e),
            })
    
    # æ˜¾ç¤ºæ±‡æ€» / Display summary
    print(f"\n{'=' * 60}")
    print("ğŸ“Š å¤„ç†æ±‡æ€» / Processing Summary")
    print(f"{'=' * 60}")
    
    success_count = sum(1 for r in results if r.get('success', False))
    print(f"æ€»ä»»åŠ¡æ•° / Total tasks: {len(results)}")
    print(f"æˆåŠŸ / Success: {success_count}")
    print(f"å¤±è´¥ / Failed: {len(results) - success_count}")
    
    if len(results) > 0:
        print(f"\nè¯¦ç»†ç»“æœ / Detailed results:")
        for r in results:
            status = "âœ…" if r.get('success', False) else "âŒ"
            print(f"   {status} Row {r['row']}: {r['task_id']}")
    
    print(f"\n{'=' * 60}")
    print(f"ğŸ‰ å¤„ç†å®Œæˆ / Processing complete!")
    print(f"{'=' * 60}\n")


if __name__ == "__main__":
    main()
